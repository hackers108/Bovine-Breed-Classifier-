{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13120412,"sourceType":"datasetVersion","datasetId":3103253},{"sourceId":13150320,"sourceType":"datasetVersion","datasetId":8331857},{"sourceId":13196421,"sourceType":"datasetVersion","datasetId":8363001},{"sourceId":591765,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":442724,"modelId":459259}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- STEP 1: SETUP & MERGE DATASETS ---\n!pip install ultralytics -q\n!pip install opencv-python pycocotools matplotlib\n\n# --- CORE IMPORTS ---\nimport os, shutil, io, random, cv2, numpy as np, pandas as pd\nfrom PIL import Image, ImageFilter\nimport matplotlib.pyplot as plt\n\n\n# --- Torchvision ---\nfrom torchvision import transforms, datasets, models\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# --- For K-Fold ---\nfrom sklearn.model_selection import StratifiedKFold\nprint(\"✅ All imports done\")\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch.optim as optim\nfrom torch.nn import CrossEntropyLoss\n\n\n#Multiprocessing\n!pip install mpire\nimport mpire as _noop  # no-op import to keep dependencies explicit (optional)\nimport multiprocessing as mp\nimport tqdm\n\n#dataset creation\nimport json\nimport subprocess, sys, glob\n\n# --- YOLO ---\nfrom ultralytics import YOLO\n\n#-----------------------------------------------------\nprint(\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:09:58.805707Z","iopub.execute_input":"2025-09-29T19:09:58.805936Z","iopub.status.idle":"2025-09-29T19:11:30.635258Z","shell.execute_reply.started":"2025-09-29T19:09:58.805918Z","shell.execute_reply":"2025-09-29T19:11:30.634426Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.10)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\n✅ All imports done\nCollecting mpire\n  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: pygments>=2.0 in /usr/local/lib/python3.11/dist-packages (from mpire) (2.19.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from mpire) (4.67.1)\nDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: mpire\nSuccessfully installed mpire-2.10.2\nCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nDone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\n# Dataset paths\nbovine_dataset = '/kaggle/input/indian-bovine-breeds/'\ncalf_dataset   = '/kaggle/input/calf-image/'\n\n# Check existence\nassert os.path.exists(bovine_dataset), \"Bovine dataset not found!\"\nassert os.path.exists(calf_dataset), \"Calf dataset not found!\"\n\n# Merged dataset base\nmerged_base = '/kaggle/working/merged_dataset'\ncsv_path = os.path.join(merged_base, \"labels.csv\")\n\n# Preprocessed dataset already exists\nif os.path.exists(merged_base) and os.path.exists(csv_path):\n    print(\"✅ Preprocessed dataset found, loading CSV...\")\n    df = pd.read_csv(csv_path)\nelse:\n    if os.path.exists(merged_base):\n        shutil.rmtree(merged_base)\n    os.makedirs(merged_base, exist_ok=True)\n    print(\"⚙ Dataset will be preprocessed for all breeds (including calves).\")\n    \n    all_rows = []\n\n    # Adult bovine images\n    for root, dirs, files in os.walk(bovine_dataset):\n        breed = os.path.basename(root)\n        if breed == os.path.basename(os.path.normpath(bovine_dataset)):\n            continue\n        for img in sorted(files):\n            if img.lower().endswith(('.jpg','.jpeg','.png')):\n                all_rows.append([os.path.join(breed, img), breed, 0])\n\n    # Calf images — ensure all images counted\n    for root, dirs, files in os.walk(calf_dataset):\n        for img in sorted(files):\n            if img.lower().endswith(('.jpg','.jpeg','.png')):\n                all_rows.append([os.path.join(\"calf\", img), \"calf\", 1])\n\n    df = pd.DataFrame(all_rows, columns=[\"image_path\",\"breed\",\"is_calf\"])\n    df.to_csv(csv_path, index=False)\n    print(f\"✅ Preprocessing complete. CSV saved at {csv_path}\")\n\n# Display number of images per breed\nprint(\"\\nNumber of images per breed (including calves):\")\ndisplay(df['breed'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:11:30.637343Z","iopub.execute_input":"2025-09-29T19:11:30.637697Z","iopub.status.idle":"2025-09-29T19:11:42.443889Z","shell.execute_reply.started":"2025-09-29T19:11:30.637677Z","shell.execute_reply":"2025-09-29T19:11:42.443340Z"}},"outputs":[{"name":"stdout","text":"⚙ Dataset will be preprocessed for all breeds (including calves).\n✅ Preprocessing complete. CSV saved at /kaggle/working/merged_dataset/labels.csv\n\nNumber of images per breed (including calves):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"breed\nSahiwal              439\nGir                  372\nHolstein_Friesian    328\nAyrshire             234\nBrown_Swiss          225\nTharparkar           217\nJersey               203\nOngole               191\nHallikar             186\nNagpuri              182\nKankrej              178\nMurrah               173\nRed_Dane             167\nRed_Sindhi           162\nRathi                149\nVechur               140\nKrishna_Valley       136\nHariana              129\nPulikulam            124\nToda                 124\nGuernsey             119\nKhillari             113\nBanni                108\nMalnad_gidda         107\nJaffrabadi           101\nAlambadi              99\nDeoni                 99\nKasargod              95\nAmritmahal            94\nMehsana               94\nBargur                93\nKangayam              91\nNili_Ravi             88\nNagori                88\nBhadawari             86\nNimari                84\nDangi                 82\nUmblachery            76\nSurti                 59\nKenkatha              55\ncalf                  47\nKherigarh             36\nName: count, dtype: int64"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# Count images per breed and sort descending\nbreed_counts = df['breed'].value_counts()\nsorted_breeds = breed_counts.index.tolist()\nsorted_counts = breed_counts.values.tolist()\n\n# Dictionary to store checkboxes\nbreed_checkboxes = {}\ncheckbox_widgets = []\n\n# Create checkbox for each breed\nfor breed, count in zip(sorted_breeds, sorted_counts):\n    cb = widgets.Checkbox(\n        value=True,  # default selected\n        description=f\"{breed} ({count} images)\",\n        indent=False\n    )\n    breed_checkboxes[breed] = cb\n    checkbox_widgets.append(cb)\n\n# Vertical box layout\nbreed_box = widgets.VBox(checkbox_widgets, layout=widgets.Layout(max_height='300px', overflow='auto'))\ndisplay(breed_box)\n\n# Button to confirm selection\nconfirm_btn = widgets.Button(description=\"Confirm Selection\", button_style='success')\nout = widgets.Output()\ndisplay(confirm_btn, out)\n\n# Internal variable to store selected breeds\nselected_breeds_for_training = []\n\ndef on_confirm_clicked(b):\n    global selected_breeds_for_training\n    selected_breeds_for_training = [breed for breed, cb in breed_checkboxes.items() if cb.value]\n    with out:\n        clear_output()\n        print(f\"✅ Selected breeds for training: {selected_breeds_for_training}\")\n\nconfirm_btn.on_click(on_confirm_clicked)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:11:42.444739Z","iopub.execute_input":"2025-09-29T19:11:42.445069Z","iopub.status.idle":"2025-09-29T19:11:42.531682Z","shell.execute_reply.started":"2025-09-29T19:11:42.445016Z","shell.execute_reply":"2025-09-29T19:11:42.530782Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Checkbox(value=True, description='Sahiwal (439 images)', indent=False), Checkbox(value=True, de…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee98cdac90374b0a9c35b858f9bf2956"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Confirm Selection', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58fbde97a91940359a46c2c2e29b381e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b4e4403bd5f4a7995feaee3718bcd56"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom ultralytics import YOLO\n\n# Function to load YOLO on a specific GPU\ndef load_yolo_on_device(device_id):\n    device = f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\"\n    model = YOLO('yolov8n.pt')  # replace with custom weights if available\n    model.to(device)\n    return model, device\n\nprint(\"✅ YOLO loader defined for multiple GPUs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:13:29.829712Z","iopub.execute_input":"2025-09-29T19:13:29.830102Z","iopub.status.idle":"2025-09-29T19:13:29.836174Z","shell.execute_reply.started":"2025-09-29T19:13:29.830050Z","shell.execute_reply":"2025-09-29T19:13:29.835537Z"}},"outputs":[{"name":"stdout","text":"✅ YOLO loader defined for multiple GPUs\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport numpy as np\n\ndef crop_with_yolo(img_path, save_path, model, device, target_size=(300,300)):\n    \"\"\"\n    Detect largest cow using YOLO and save cropped image.\n    \"\"\"\n    image = cv2.imread(img_path)\n    if image is None:\n        print(f\"⚠ Could not read image: {img_path}\")\n        return False\n\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    results = model.predict(image_rgb, device=device, verbose=False)[0]\n\n    boxes = results.boxes.xyxy.cpu().numpy() if len(results.boxes) > 0 else []\n    if len(boxes) == 0:\n        return False\n\n    # Take the largest box\n    areas = [(x2-x1)*(y2-y1) for x1,y1,x2,y2 in boxes]\n    largest_idx = np.argmax(areas)\n    x1,y1,x2,y2 = [int(c) for c in boxes[largest_idx]]\n\n    crop = image[y1:y2, x1:x2]\n    if crop.size == 0:\n        return False\n\n    crop_resized = cv2.resize(crop, target_size)\n    Image.fromarray(cv2.cvtColor(crop_resized, cv2.COLOR_BGR2RGB)).save(save_path)\n    return True\n\nprint(\"✅ Crop function ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:13:33.436852Z","iopub.execute_input":"2025-09-29T19:13:33.437201Z","iopub.status.idle":"2025-09-29T19:13:33.444660Z","shell.execute_reply.started":"2025-09-29T19:13:33.437180Z","shell.execute_reply":"2025-09-29T19:13:33.443942Z"}},"outputs":[{"name":"stdout","text":"✅ Crop function ready\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#-----not needed---\ndef process_image(args):\n    src, dst, breed, is_calf, device_id = args\n    try:\n        mask_generator, device = load_sam_on_device(device_id)\n        if crop_with_sam(src, dst, mask_generator, device, max_dim=512):\n            return [os.path.join(breed, os.path.basename(dst)), breed, is_calf]\n    except Exception as e:\n        print(f\"⚠️ Error {src}: {e}\")\n    return None\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:13:38.845698Z","iopub.execute_input":"2025-09-29T19:13:38.846263Z","iopub.status.idle":"2025-09-29T19:13:38.851410Z","shell.execute_reply.started":"2025-09-29T19:13:38.846238Z","shell.execute_reply":"2025-09-29T19:13:38.850711Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import shutil\nimport tqdm\nimport pandas as pd\n\n# Merged dataset base\nmerged_base = '/kaggle/working/merged_dataset'\ncsv_path = os.path.join(merged_base, \"labels.csv\")\n\nos.makedirs(merged_base, exist_ok=True)\nall_tasks = []\n\n# Adult bovine images\nfor root, dirs, files in os.walk(bovine_dataset):\n    breed = os.path.basename(root)\n    if breed not in selected_breeds_for_training:\n        continue\n    target_dir = os.path.join(merged_base, breed)\n    os.makedirs(target_dir, exist_ok=True)\n    for img in sorted(files):\n        if img.lower().endswith(('.jpg','.jpeg','.png')):\n            src = os.path.join(root,img)\n            dst = os.path.join(target_dir,img)\n            all_tasks.append((src,dst,breed,0))\n\n# Calf images\nif 'calf' in selected_breeds_for_training:\n    calf_target = os.path.join(merged_base,'calf')\n    os.makedirs(calf_target, exist_ok=True)\n    for img in sorted(os.listdir(calf_dataset)):\n        if img.lower().endswith(('.jpg','.jpeg','.png')):\n            src = os.path.join(calf_dataset,img)\n            dst = os.path.join(calf_target,img)\n            all_tasks.append((src,dst,'calf',1))\n\nprint(f\"Total images to process: {len(all_tasks)}\")\n\n# Distribute tasks across GPUs\ngpu_count = torch.cuda.device_count()\nif gpu_count == 0:\n    raise RuntimeError(\"No CUDA GPUs found.\")\ntasks_per_gpu = {i:[] for i in range(gpu_count)}\nfor i, task in enumerate(all_tasks):\n    gpu_id = i % gpu_count\n    tasks_per_gpu[gpu_id].append(task)\n\nprint(f\"✅ Tasks distributed across {gpu_count} GPUs\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:13:41.923110Z","iopub.execute_input":"2025-09-29T19:13:41.923385Z","iopub.status.idle":"2025-09-29T19:13:44.785249Z","shell.execute_reply.started":"2025-09-29T19:13:41.923367Z","shell.execute_reply":"2025-09-29T19:13:44.784629Z"}},"outputs":[{"name":"stdout","text":"Total images to process: 2928\n✅ Tasks distributed across 2 GPUs\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"all_rows = []\n\nfor gpu_id in range(gpu_count):\n    tasks = tasks_per_gpu[gpu_id]\n    if not tasks:\n        print(f\"GPU {gpu_id} has 0 tasks — skipping.\")\n        continue\n\n    print(f\"\\n--- Processing {len(tasks)} images on GPU {gpu_id} ---\")\n    try:\n        torch.cuda.set_device(gpu_id)\n    except Exception:\n        pass\n\n    # Load YOLO model on this GPU\n    model, device = load_yolo_on_device(gpu_id)\n\n    pbar = tqdm.tqdm(tasks, desc=f\"GPU {gpu_id}\", position=gpu_id, leave=True)\n    for src,dst,breed,is_calf in pbar:\n        try:\n            ok = crop_with_yolo(src,dst,model,device)\n            if ok:\n                all_rows.append([os.path.join(breed, os.path.basename(dst)), breed, is_calf])\n        except Exception as e:\n            print(f\"⚠ Error processing {src}: {e}\")\n\n    # Free GPU memory\n    try:\n        del model\n        torch.cuda.empty_cache()\n    except Exception:\n        pass\n\n# Save CSV\nif all_rows:\n    df_out = pd.DataFrame(all_rows, columns=[\"image_path\",\"breed\",\"is_calf\"])\n    df_out.to_csv(csv_path, index=False)\n    df = df_out\n    print(\"✅ Preprocessing complete — CSV saved at:\", csv_path)\n    print(df['breed'].value_counts())\nelse:\n    print(\"⚠ No images were successfully processed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:16:01.205941Z","iopub.execute_input":"2025-09-29T19:16:01.206699Z","iopub.status.idle":"2025-09-29T19:17:23.367399Z","shell.execute_reply.started":"2025-09-29T19:16:01.206675Z","shell.execute_reply":"2025-09-29T19:17:23.366750Z"}},"outputs":[{"name":"stdout","text":"\n--- Processing 1464 images on GPU 0 ---\n","output_type":"stream"},{"name":"stderr","text":"GPU 0:  89%|████████▉ | 1303/1464 [00:34<00:02, 62.77it/s]libpng warning: iCCP: known incorrect sRGB profile\nGPU 0:  98%|█████████▊| 1440/1464 [00:40<00:01, 23.76it/s]libpng warning: iCCP: known incorrect sRGB profile\nGPU 0: 100%|██████████| 1464/1464 [00:40<00:00, 35.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n--- Processing 1464 images on GPU 1 ---\n","output_type":"stream"},{"name":"stderr","text":"\nGPU 1:   0%|          | 0/1464 [00:00<?, ?it/s]\u001b[A\nGPU 1:   0%|          | 3/1464 [00:00<00:57, 25.19it/s]\u001b[A\nGPU 1:   0%|          | 7/1464 [00:00<00:55, 26.22it/s]\u001b[A\nGPU 1:   1%|          | 11/1464 [00:00<01:00, 24.01it/s]\u001b[A\nGPU 1:   1%|          | 17/1464 [00:00<00:42, 34.31it/s]\u001b[A\nGPU 1:   1%|▏         | 21/1464 [00:00<00:42, 34.14it/s]\u001b[A\nGPU 1:   2%|▏         | 25/1464 [00:00<00:46, 30.91it/s]\u001b[A\nGPU 1:   2%|▏         | 32/1464 [00:00<00:35, 40.42it/s]\u001b[A\nGPU 1:   3%|▎         | 37/1464 [00:01<00:41, 34.60it/s]\u001b[A\nGPU 1:   3%|▎         | 42/1464 [00:01<00:42, 33.33it/s]\u001b[A\nGPU 1:   3%|▎         | 46/1464 [00:01<00:41, 34.42it/s]\u001b[A\nGPU 1:   4%|▎         | 53/1464 [00:01<00:33, 42.12it/s]\u001b[A\nGPU 1:   4%|▍         | 59/1464 [00:01<00:30, 45.88it/s]\u001b[A\nGPU 1:   5%|▍         | 66/1464 [00:01<00:27, 50.11it/s]\u001b[A\nGPU 1:   5%|▍         | 72/1464 [00:01<00:26, 52.26it/s]\u001b[A\nGPU 1:   5%|▌         | 78/1464 [00:01<00:28, 49.02it/s]\u001b[A\nGPU 1:   6%|▌         | 84/1464 [00:02<00:27, 50.90it/s]\u001b[A\nGPU 1:   6%|▌         | 90/1464 [00:02<00:27, 49.44it/s]\u001b[A\nGPU 1:   7%|▋         | 96/1464 [00:02<00:29, 46.01it/s]\u001b[A\nGPU 1:   7%|▋         | 101/1464 [00:02<00:30, 45.38it/s]\u001b[A\nGPU 1:   7%|▋         | 107/1464 [00:02<00:27, 48.95it/s]\u001b[A\nGPU 1:   8%|▊         | 114/1464 [00:02<00:25, 52.89it/s]\u001b[A\nGPU 1:   8%|▊         | 120/1464 [00:02<00:30, 44.42it/s]\u001b[A\nGPU 1:   9%|▊         | 125/1464 [00:03<00:33, 39.83it/s]\u001b[A\nGPU 1:   9%|▉         | 130/1464 [00:03<00:52, 25.35it/s]\u001b[A\nGPU 1:   9%|▉         | 134/1464 [00:03<00:59, 22.41it/s]\u001b[A\nGPU 1:   9%|▉         | 137/1464 [00:03<00:58, 22.62it/s]\u001b[A\nGPU 1:  10%|▉         | 140/1464 [00:03<00:59, 22.26it/s]\u001b[A\nGPU 1:  10%|▉         | 143/1464 [00:04<01:14, 17.85it/s]\u001b[A\nGPU 1:  10%|▉         | 146/1464 [00:04<01:10, 18.64it/s]\u001b[A\nGPU 1:  10%|█         | 149/1464 [00:04<01:18, 16.75it/s]\u001b[A\nGPU 1:  10%|█         | 151/1464 [00:04<01:19, 16.54it/s]\u001b[A\nGPU 1:  10%|█         | 153/1464 [00:04<01:19, 16.44it/s]\u001b[A\nGPU 1:  11%|█         | 155/1464 [00:05<01:32, 14.22it/s]\u001b[A\nGPU 1:  11%|█         | 157/1464 [00:05<01:28, 14.69it/s]\u001b[A\nGPU 1:  11%|█         | 164/1464 [00:05<00:51, 25.31it/s]\u001b[A\nGPU 1:  12%|█▏        | 170/1464 [00:05<00:39, 32.58it/s]\u001b[A\nGPU 1:  12%|█▏        | 177/1464 [00:05<00:32, 39.68it/s]\u001b[A\nGPU 1:  12%|█▎        | 183/1464 [00:05<00:28, 44.28it/s]\u001b[A\nGPU 1:  13%|█▎        | 188/1464 [00:05<00:34, 37.31it/s]\u001b[A\nGPU 1:  13%|█▎        | 193/1464 [00:05<00:38, 33.30it/s]\u001b[A\nGPU 1:  14%|█▎        | 198/1464 [00:06<00:34, 36.39it/s]\u001b[A\nGPU 1:  14%|█▍        | 203/1464 [00:06<00:33, 37.41it/s]\u001b[A\nGPU 1:  14%|█▍        | 209/1464 [00:06<00:30, 41.37it/s]\u001b[A\nGPU 1:  15%|█▍        | 214/1464 [00:06<00:41, 30.37it/s]\u001b[A\nGPU 1:  15%|█▍        | 218/1464 [00:06<00:43, 28.50it/s]\u001b[A\nGPU 1:  15%|█▌        | 222/1464 [00:06<00:45, 27.40it/s]\u001b[A\nGPU 1:  16%|█▌        | 228/1464 [00:07<00:37, 32.76it/s]\u001b[A\nGPU 1:  16%|█▌        | 232/1464 [00:07<00:44, 27.77it/s]\u001b[A\nGPU 1:  16%|█▌        | 236/1464 [00:07<00:40, 30.14it/s]\u001b[A\nGPU 1:  16%|█▋        | 240/1464 [00:07<00:42, 29.01it/s]\u001b[A\nGPU 1:  17%|█▋        | 246/1464 [00:07<00:35, 34.51it/s]\u001b[A\nGPU 1:  17%|█▋        | 250/1464 [00:07<00:39, 30.36it/s]\u001b[A\nGPU 1:  17%|█▋        | 256/1464 [00:07<00:35, 33.77it/s]\u001b[A\nGPU 1:  18%|█▊        | 260/1464 [00:08<00:35, 33.72it/s]\u001b[A\nGPU 1:  18%|█▊        | 265/1464 [00:08<00:40, 29.80it/s]\u001b[A\nGPU 1:  18%|█▊        | 269/1464 [00:08<00:54, 21.94it/s]\u001b[A\nGPU 1:  19%|█▊        | 272/1464 [00:08<01:16, 15.51it/s]\u001b[A\nGPU 1:  19%|█▉        | 275/1464 [00:09<01:20, 14.85it/s]\u001b[A\nGPU 1:  19%|█▉        | 277/1464 [00:09<01:18, 15.11it/s]\u001b[A\nGPU 1:  19%|█▉        | 279/1464 [00:09<01:30, 13.04it/s]\u001b[A\nGPU 1:  19%|█▉        | 281/1464 [00:09<01:37, 12.09it/s]\u001b[A\nGPU 1:  19%|█▉        | 284/1464 [00:09<01:28, 13.31it/s]\u001b[A\nGPU 1:  20%|█▉        | 286/1464 [00:10<01:33, 12.58it/s]\u001b[A\nGPU 1:  20%|█▉        | 288/1464 [00:10<01:42, 11.49it/s]\u001b[A\nGPU 1:  20%|█▉        | 290/1464 [00:10<01:47, 10.95it/s]\u001b[A\nGPU 1:  20%|█▉        | 292/1464 [00:10<01:36, 12.12it/s]\u001b[A\nGPU 1:  20%|██        | 294/1464 [00:10<01:30, 12.94it/s]\u001b[A\nGPU 1:  20%|██        | 296/1464 [00:10<01:23, 13.91it/s]\u001b[A\nGPU 1:  20%|██        | 298/1464 [00:11<01:20, 14.47it/s]\u001b[A\nGPU 1:  21%|██        | 302/1464 [00:11<00:59, 19.49it/s]\u001b[A\nGPU 1:  21%|██        | 309/1464 [00:11<00:36, 31.38it/s]\u001b[A\nGPU 1:  22%|██▏       | 317/1464 [00:11<00:26, 43.19it/s]\u001b[A\nGPU 1:  22%|██▏       | 326/1464 [00:11<00:20, 54.40it/s]\u001b[A\nGPU 1:  23%|██▎       | 335/1464 [00:11<00:18, 61.96it/s]\u001b[A\nGPU 1:  23%|██▎       | 344/1464 [00:11<00:16, 68.15it/s]\u001b[A\nGPU 1:  24%|██▍       | 353/1464 [00:11<00:15, 72.92it/s]\u001b[A\nGPU 1:  25%|██▍       | 362/1464 [00:11<00:14, 75.46it/s]\u001b[A\nGPU 1:  25%|██▌       | 370/1464 [00:12<00:14, 76.70it/s]\u001b[A\nGPU 1:  26%|██▌       | 379/1464 [00:12<00:13, 78.46it/s]\u001b[A\nGPU 1:  26%|██▋       | 387/1464 [00:12<00:13, 78.36it/s]\u001b[A\nGPU 1:  27%|██▋       | 395/1464 [00:12<00:13, 78.68it/s]\u001b[A\nGPU 1:  28%|██▊       | 403/1464 [00:12<00:13, 78.11it/s]\u001b[A\nGPU 1:  28%|██▊       | 412/1464 [00:12<00:13, 79.56it/s]\u001b[A\nGPU 1:  29%|██▉       | 421/1464 [00:12<00:13, 80.04it/s]\u001b[Alibpng warning: iCCP: known incorrect sRGB profile\n\nGPU 1:  29%|██▉       | 430/1464 [00:12<00:16, 64.47it/s]\u001b[A\nGPU 1:  30%|██▉       | 437/1464 [00:12<00:15, 65.33it/s]\u001b[A\nGPU 1:  30%|███       | 444/1464 [00:13<00:18, 54.56it/s]\u001b[A\nGPU 1:  31%|███       | 450/1464 [00:13<00:20, 49.69it/s]\u001b[A\nGPU 1:  31%|███       | 456/1464 [00:13<00:23, 43.70it/s]\u001b[A\nGPU 1:  31%|███▏      | 461/1464 [00:13<00:23, 42.72it/s]\u001b[Alibpng warning: iCCP: profile 'ICC Profile': 0h: PCS illuminant is not D50\n\nGPU 1:  32%|███▏      | 466/1464 [00:13<00:24, 41.39it/s]\u001b[A\nGPU 1:  32%|███▏      | 472/1464 [00:13<00:22, 44.47it/s]\u001b[A\nGPU 1:  33%|███▎      | 477/1464 [00:13<00:22, 44.54it/s]\u001b[A\nGPU 1:  33%|███▎      | 482/1464 [00:14<00:23, 41.87it/s]\u001b[A\nGPU 1:  33%|███▎      | 487/1464 [00:14<00:23, 41.95it/s]\u001b[Alibpng warning: iCCP: known incorrect sRGB profile\n\nGPU 1:  34%|███▎      | 492/1464 [00:14<00:25, 38.03it/s]\u001b[A\nGPU 1:  34%|███▍      | 496/1464 [00:14<00:26, 36.16it/s]\u001b[A\nGPU 1:  34%|███▍      | 500/1464 [00:14<00:26, 36.64it/s]\u001b[A\nGPU 1:  35%|███▍      | 506/1464 [00:14<00:22, 42.09it/s]\u001b[A\nGPU 1:  35%|███▌      | 513/1464 [00:14<00:19, 47.66it/s]\u001b[A\nGPU 1:  36%|███▌      | 520/1464 [00:14<00:17, 52.47it/s]\u001b[A\nGPU 1:  36%|███▌      | 527/1464 [00:15<00:16, 55.28it/s]\u001b[A\nGPU 1:  36%|███▋      | 533/1464 [00:15<00:18, 51.22it/s]\u001b[A\nGPU 1:  37%|███▋      | 540/1464 [00:15<00:16, 54.53it/s]\u001b[A\nGPU 1:  37%|███▋      | 548/1464 [00:15<00:15, 58.93it/s]\u001b[A\nGPU 1:  38%|███▊      | 555/1464 [00:15<00:14, 61.57it/s]\u001b[A\nGPU 1:  38%|███▊      | 562/1464 [00:15<00:14, 62.88it/s]\u001b[A\nGPU 1:  39%|███▉      | 570/1464 [00:15<00:13, 66.00it/s]\u001b[A\nGPU 1:  39%|███▉      | 577/1464 [00:15<00:13, 64.61it/s]\u001b[A\nGPU 1:  40%|███▉      | 584/1464 [00:15<00:14, 61.23it/s]\u001b[A\nGPU 1:  40%|████      | 591/1464 [00:16<00:17, 49.88it/s]\u001b[A\nGPU 1:  41%|████      | 597/1464 [00:16<00:28, 29.93it/s]\u001b[A\nGPU 1:  41%|████      | 602/1464 [00:16<00:28, 30.03it/s]\u001b[A\nGPU 1:  41%|████▏     | 607/1464 [00:16<00:25, 33.05it/s]\u001b[A\nGPU 1:  42%|████▏     | 613/1464 [00:16<00:22, 37.92it/s]\u001b[A\nGPU 1:  42%|████▏     | 618/1464 [00:17<00:24, 34.09it/s]\u001b[A\nGPU 1:  43%|████▎     | 624/1464 [00:17<00:21, 39.24it/s]\u001b[A\nGPU 1:  43%|████▎     | 632/1464 [00:17<00:17, 47.67it/s]\u001b[A\nGPU 1:  44%|████▎     | 638/1464 [00:17<00:19, 43.14it/s]\u001b[A\nGPU 1:  44%|████▍     | 643/1464 [00:17<00:20, 39.65it/s]\u001b[A\nGPU 1:  44%|████▍     | 650/1464 [00:17<00:17, 46.13it/s]\u001b[A\nGPU 1:  45%|████▍     | 656/1464 [00:18<00:26, 30.33it/s]\u001b[A\nGPU 1:  45%|████▌     | 661/1464 [00:18<00:33, 24.09it/s]\u001b[A\nGPU 1:  45%|████▌     | 665/1464 [00:18<00:37, 21.53it/s]\u001b[A\nGPU 1:  46%|████▌     | 668/1464 [00:18<00:41, 19.12it/s]\u001b[A\nGPU 1:  46%|████▌     | 671/1464 [00:19<00:44, 17.86it/s]\u001b[A\nGPU 1:  46%|████▌     | 674/1464 [00:19<00:49, 16.03it/s]\u001b[A\nGPU 1:  46%|████▌     | 676/1464 [00:19<00:56, 13.95it/s]\u001b[A\nGPU 1:  46%|████▋     | 678/1464 [00:19<01:01, 12.68it/s]\u001b[A\nGPU 1:  46%|████▋     | 680/1464 [00:20<01:05, 11.95it/s]\u001b[A\nGPU 1:  47%|████▋     | 682/1464 [00:20<01:09, 11.28it/s]\u001b[A\nGPU 1:  47%|████▋     | 685/1464 [00:20<00:58, 13.31it/s]\u001b[A\nGPU 1:  47%|████▋     | 687/1464 [00:20<01:04, 12.02it/s]\u001b[A\nGPU 1:  47%|████▋     | 689/1464 [00:20<01:08, 11.34it/s]\u001b[A\nGPU 1:  47%|████▋     | 695/1464 [00:20<00:38, 19.79it/s]\u001b[A\nGPU 1:  48%|████▊     | 702/1464 [00:21<00:26, 29.10it/s]\u001b[A\nGPU 1:  48%|████▊     | 709/1464 [00:21<00:23, 31.53it/s]\u001b[A\nGPU 1:  49%|████▉     | 716/1464 [00:21<00:22, 33.44it/s]\u001b[A\nGPU 1:  49%|████▉     | 720/1464 [00:21<00:23, 31.94it/s]\u001b[A\nGPU 1:  49%|████▉     | 724/1464 [00:21<00:30, 24.40it/s]\u001b[A\nGPU 1:  50%|████▉     | 728/1464 [00:22<00:30, 24.35it/s]\u001b[A\nGPU 1:  50%|████▉     | 731/1464 [00:22<00:35, 20.42it/s]\u001b[A\nGPU 1:  50%|█████     | 734/1464 [00:22<00:40, 18.12it/s]\u001b[A\nGPU 1:  50%|█████     | 736/1464 [00:22<00:41, 17.60it/s]\u001b[A\nGPU 1:  50%|█████     | 738/1464 [00:22<00:42, 17.24it/s]\u001b[A\nGPU 1:  51%|█████     | 741/1464 [00:22<00:38, 18.94it/s]\u001b[A\nGPU 1:  51%|█████     | 743/1464 [00:22<00:37, 19.14it/s]\u001b[A\nGPU 1:  51%|█████     | 746/1464 [00:23<00:33, 21.69it/s]\u001b[A\nGPU 1:  51%|█████     | 750/1464 [00:23<00:27, 25.68it/s]\u001b[A\nGPU 1:  52%|█████▏    | 756/1464 [00:23<00:21, 32.78it/s]\u001b[A\nGPU 1:  52%|█████▏    | 760/1464 [00:23<00:25, 27.11it/s]\u001b[A\nGPU 1:  52%|█████▏    | 763/1464 [00:23<00:33, 21.20it/s]\u001b[A\nGPU 1:  52%|█████▏    | 766/1464 [00:23<00:32, 21.66it/s]\u001b[A\nGPU 1:  53%|█████▎    | 771/1464 [00:24<00:29, 23.84it/s]\u001b[A\nGPU 1:  53%|█████▎    | 774/1464 [00:24<00:33, 20.71it/s]\u001b[A\nGPU 1:  53%|█████▎    | 778/1464 [00:24<00:28, 24.31it/s]\u001b[A\nGPU 1:  53%|█████▎    | 782/1464 [00:24<00:24, 27.55it/s]\u001b[A\nGPU 1:  54%|█████▎    | 786/1464 [00:24<00:24, 27.79it/s]\u001b[A\nGPU 1:  54%|█████▍    | 793/1464 [00:24<00:18, 37.12it/s]\u001b[A\nGPU 1:  55%|█████▍    | 798/1464 [00:24<00:19, 34.97it/s]\u001b[A\nGPU 1:  55%|█████▌    | 806/1464 [00:24<00:14, 44.78it/s]\u001b[A\nGPU 1:  56%|█████▌    | 814/1464 [00:25<00:12, 53.23it/s]\u001b[A\nGPU 1:  56%|█████▌    | 823/1464 [00:25<00:10, 61.75it/s]\u001b[A\nGPU 1:  57%|█████▋    | 832/1464 [00:25<00:09, 67.32it/s]\u001b[A\nGPU 1:  57%|█████▋    | 841/1464 [00:25<00:08, 71.35it/s]\u001b[A\nGPU 1:  58%|█████▊    | 849/1464 [00:25<00:08, 73.27it/s]\u001b[A\nGPU 1:  59%|█████▊    | 857/1464 [00:25<00:08, 74.72it/s]\u001b[A\nGPU 1:  59%|█████▉    | 865/1464 [00:25<00:07, 75.41it/s]\u001b[A\nGPU 1:  60%|█████▉    | 873/1464 [00:25<00:07, 76.52it/s]\u001b[A\nGPU 1:  60%|██████    | 882/1464 [00:25<00:07, 78.24it/s]\u001b[A\nGPU 1:  61%|██████    | 891/1464 [00:26<00:07, 79.56it/s]\u001b[A\nGPU 1:  61%|██████▏   | 900/1464 [00:26<00:07, 80.31it/s]\u001b[A\nGPU 1:  62%|██████▏   | 909/1464 [00:26<00:06, 79.94it/s]\u001b[A\nGPU 1:  63%|██████▎   | 918/1464 [00:26<00:06, 79.17it/s]\u001b[A\nGPU 1:  63%|██████▎   | 926/1464 [00:26<00:14, 36.98it/s]\u001b[A\nGPU 1:  64%|██████▎   | 933/1464 [00:27<00:17, 30.28it/s]\u001b[A\nGPU 1:  64%|██████▍   | 938/1464 [00:27<00:16, 32.21it/s]\u001b[A\nGPU 1:  64%|██████▍   | 943/1464 [00:27<00:16, 31.23it/s]\u001b[A\nGPU 1:  65%|██████▍   | 948/1464 [00:27<00:15, 34.23it/s]\u001b[A\nGPU 1:  65%|██████▌   | 953/1464 [00:27<00:15, 33.88it/s]\u001b[A\nGPU 1:  66%|██████▌   | 960/1464 [00:27<00:12, 38.78it/s]\u001b[A\nGPU 1:  66%|██████▌   | 967/1464 [00:28<00:11, 43.96it/s]\u001b[A\nGPU 1:  66%|██████▋   | 972/1464 [00:28<00:17, 28.71it/s]\u001b[A\nGPU 1:  67%|██████▋   | 976/1464 [00:28<00:17, 27.46it/s]\u001b[A\nGPU 1:  67%|██████▋   | 980/1464 [00:28<00:23, 20.21it/s]\u001b[A\nGPU 1:  67%|██████▋   | 983/1464 [00:29<00:29, 16.41it/s]\u001b[A\nGPU 1:  67%|██████▋   | 986/1464 [00:29<00:34, 13.93it/s]\u001b[A\nGPU 1:  67%|██████▋   | 988/1464 [00:29<00:33, 14.15it/s]\u001b[A\nGPU 1:  68%|██████▊   | 990/1464 [00:29<00:32, 14.60it/s]\u001b[A\nGPU 1:  68%|██████▊   | 992/1464 [00:29<00:31, 15.10it/s]\u001b[A\nGPU 1:  68%|██████▊   | 994/1464 [00:30<00:35, 13.18it/s]\u001b[A\nGPU 1:  68%|██████▊   | 997/1464 [00:30<00:31, 14.97it/s]\u001b[A\nGPU 1:  68%|██████▊   | 999/1464 [00:30<00:30, 15.07it/s]\u001b[A\nGPU 1:  68%|██████▊   | 1001/1464 [00:30<00:34, 13.50it/s]\u001b[A\nGPU 1:  69%|██████▊   | 1003/1464 [00:30<00:34, 13.25it/s]\u001b[A\nGPU 1:  69%|██████▊   | 1005/1464 [00:30<00:37, 12.39it/s]\u001b[A\nGPU 1:  69%|██████▉   | 1007/1464 [00:31<00:35, 12.84it/s]\u001b[A\nGPU 1:  69%|██████▉   | 1009/1464 [00:31<00:39, 11.47it/s]\u001b[A\nGPU 1:  69%|██████▉   | 1011/1464 [00:31<00:35, 12.80it/s]\u001b[A\nGPU 1:  69%|██████▉   | 1013/1464 [00:31<00:32, 13.88it/s]\u001b[A\nGPU 1:  70%|██████▉   | 1021/1464 [00:31<00:15, 28.53it/s]\u001b[A\nGPU 1:  70%|███████   | 1029/1464 [00:31<00:10, 40.21it/s]\u001b[A\nGPU 1:  71%|███████   | 1037/1464 [00:31<00:08, 49.09it/s]\u001b[A\nGPU 1:  71%|███████▏  | 1045/1464 [00:31<00:07, 56.87it/s]\u001b[A\nGPU 1:  72%|███████▏  | 1054/1464 [00:32<00:06, 64.55it/s]\u001b[A\nGPU 1:  73%|███████▎  | 1063/1464 [00:32<00:05, 70.25it/s]\u001b[A\nGPU 1:  73%|███████▎  | 1072/1464 [00:32<00:05, 73.51it/s]\u001b[A\nGPU 1:  74%|███████▍  | 1081/1464 [00:32<00:05, 76.25it/s]\u001b[A\nGPU 1:  74%|███████▍  | 1090/1464 [00:32<00:04, 77.61it/s]\u001b[A\nGPU 1:  75%|███████▌  | 1098/1464 [00:32<00:04, 78.15it/s]\u001b[A\nGPU 1:  76%|███████▌  | 1107/1464 [00:32<00:04, 79.95it/s]\u001b[A\nGPU 1:  76%|███████▌  | 1116/1464 [00:32<00:04, 80.23it/s]\u001b[A\nGPU 1:  77%|███████▋  | 1125/1464 [00:32<00:04, 81.47it/s]\u001b[A\nGPU 1:  77%|███████▋  | 1134/1464 [00:33<00:04, 80.33it/s]\u001b[A\nGPU 1:  78%|███████▊  | 1143/1464 [00:33<00:03, 80.68it/s]\u001b[A\nGPU 1:  79%|███████▊  | 1152/1464 [00:33<00:03, 81.84it/s]\u001b[A\nGPU 1:  79%|███████▉  | 1161/1464 [00:33<00:03, 82.04it/s]\u001b[A\nGPU 1:  80%|███████▉  | 1170/1464 [00:33<00:03, 81.83it/s]\u001b[A\nGPU 1:  81%|████████  | 1179/1464 [00:33<00:03, 81.25it/s]\u001b[A\nGPU 1:  81%|████████  | 1188/1464 [00:33<00:03, 81.01it/s]\u001b[A\nGPU 1:  82%|████████▏ | 1197/1464 [00:33<00:03, 81.01it/s]\u001b[A\nGPU 1:  82%|████████▏ | 1206/1464 [00:33<00:03, 82.72it/s]\u001b[A\nGPU 1:  83%|████████▎ | 1215/1464 [00:34<00:03, 81.54it/s]\u001b[A\nGPU 1:  84%|████████▎ | 1224/1464 [00:34<00:02, 81.69it/s]\u001b[A\nGPU 1:  84%|████████▍ | 1233/1464 [00:34<00:02, 82.57it/s]\u001b[A\nGPU 1:  85%|████████▍ | 1242/1464 [00:34<00:02, 82.05it/s]\u001b[A\nGPU 1:  85%|████████▌ | 1251/1464 [00:34<00:02, 82.01it/s]\u001b[A\nGPU 1:  86%|████████▌ | 1260/1464 [00:34<00:02, 81.37it/s]\u001b[A\nGPU 1:  87%|████████▋ | 1269/1464 [00:34<00:02, 81.89it/s]\u001b[A\nGPU 1:  87%|████████▋ | 1278/1464 [00:34<00:02, 82.28it/s]\u001b[A\nGPU 1:  88%|████████▊ | 1287/1464 [00:35<00:03, 48.40it/s]\u001b[A\nGPU 1:  88%|████████▊ | 1294/1464 [00:35<00:03, 45.80it/s]\u001b[A\nGPU 1:  89%|████████▉ | 1301/1464 [00:35<00:03, 48.65it/s]\u001b[A\nGPU 1:  89%|████████▉ | 1307/1464 [00:35<00:03, 50.65it/s]\u001b[A\nGPU 1:  90%|████████▉ | 1313/1464 [00:35<00:03, 45.11it/s]\u001b[A\nGPU 1:  90%|█████████ | 1319/1464 [00:35<00:03, 40.31it/s]\u001b[A\nGPU 1:  90%|█████████ | 1324/1464 [00:36<00:03, 35.63it/s]\u001b[A\nGPU 1:  91%|█████████ | 1328/1464 [00:36<00:04, 32.91it/s]\u001b[A\nGPU 1:  91%|█████████ | 1332/1464 [00:36<00:04, 27.22it/s]\u001b[A\nGPU 1:  91%|█████████▏| 1338/1464 [00:36<00:03, 32.90it/s]\u001b[A\nGPU 1:  92%|█████████▏| 1345/1464 [00:36<00:02, 40.01it/s]\u001b[A\nGPU 1:  92%|█████████▏| 1351/1464 [00:36<00:02, 43.05it/s]\u001b[A\nGPU 1:  93%|█████████▎| 1356/1464 [00:37<00:03, 34.14it/s]\u001b[A\nGPU 1:  93%|█████████▎| 1362/1464 [00:37<00:02, 38.64it/s]\u001b[A\nGPU 1:  94%|█████████▎| 1369/1464 [00:37<00:02, 44.88it/s]\u001b[A\nGPU 1:  94%|█████████▍| 1375/1464 [00:37<00:02, 41.03it/s]\u001b[A\nGPU 1:  94%|█████████▍| 1380/1464 [00:37<00:02, 40.68it/s]\u001b[A\nGPU 1:  95%|█████████▍| 1387/1464 [00:37<00:01, 46.86it/s]\u001b[A\nGPU 1:  95%|█████████▌| 1393/1464 [00:37<00:01, 48.41it/s]\u001b[A\nGPU 1:  96%|█████████▌| 1399/1464 [00:38<00:02, 27.88it/s]\u001b[A\nGPU 1:  96%|█████████▌| 1403/1464 [00:38<00:02, 27.77it/s]\u001b[A\nGPU 1:  96%|█████████▌| 1407/1464 [00:38<00:02, 23.38it/s]\u001b[A\nGPU 1:  96%|█████████▋| 1410/1464 [00:38<00:02, 20.39it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1413/1464 [00:39<00:03, 17.00it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1416/1464 [00:39<00:02, 17.48it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1419/1464 [00:39<00:02, 17.41it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1421/1464 [00:39<00:02, 15.87it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1423/1464 [00:39<00:02, 15.53it/s]\u001b[A\nGPU 1:  97%|█████████▋| 1426/1464 [00:40<00:02, 14.46it/s]\u001b[A\nGPU 1:  98%|█████████▊| 1429/1464 [00:40<00:02, 15.56it/s]\u001b[A\nGPU 1:  98%|█████████▊| 1431/1464 [00:40<00:02, 15.37it/s]\u001b[A\nGPU 1:  98%|█████████▊| 1433/1464 [00:40<00:01, 15.66it/s]\u001b[A\nGPU 1:  98%|█████████▊| 1438/1464 [00:40<00:01, 23.08it/s]\u001b[A\nGPU 1:  98%|█████████▊| 1442/1464 [00:40<00:00, 26.20it/s]\u001b[A\nGPU 1:  99%|█████████▉| 1448/1464 [00:40<00:00, 33.24it/s]\u001b[A\nGPU 1:  99%|█████████▉| 1452/1464 [00:40<00:00, 33.36it/s]\u001b[A\nGPU 1:  99%|█████████▉| 1456/1464 [00:41<00:00, 27.71it/s]\u001b[A\nGPU 1: 100%|██████████| 1464/1464 [00:41<00:00, 35.46it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"✅ Preprocessing complete — CSV saved at: /kaggle/working/merged_dataset/labels.csv\nbreed\nSahiwal              427\nGir                  369\nHolstein_Friesian    309\nAyrshire             222\nTharparkar           208\nBrown_Swiss          206\nOngole               187\nJersey               184\nHallikar             180\nNagpuri              175\nKankrej              173\nMurrah               171\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class RandomImageQuality:\n    def __init__(self, p_blur=0.5, p_jpeg=0.5):\n        self.p_blur = p_blur\n        self.p_jpeg = p_jpeg\n\n    def __call__(self, img):\n        # Random blur\n        if random.random() < self.p_blur:\n            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 2.0)))\n\n        # Random JPEG compression\n        if random.random() < self.p_jpeg:\n            buffer = io.BytesIO()\n            quality = random.randint(30, 90)\n            img.save(buffer, format='JPEG', quality=quality)\n            buffer.seek(0)\n            img = Image.open(buffer)\n\n        return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:19:14.929288Z","iopub.status.idle":"2025-09-29T19:19:14.929487Z","shell.execute_reply.started":"2025-09-29T19:19:14.929391Z","shell.execute_reply":"2025-09-29T19:19:14.929399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport random\nimport torchvision.transforms as transforms\nimport torch\n\nclass RandomOcclusion:\n    def __init__(self, p=0.5, size=(20,50)):\n        self.p = p          # probability of applying occlusion\n        self.size = size    # min-max size of occlusion block in pixels\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            w, h = img.size\n            # Random width and height of obstruction\n            occ_w = random.randint(self.size[0], self.size[1])\n            occ_h = random.randint(self.size[0], self.size[1])\n            # Random top-left position\n            x1 = random.randint(0, w - occ_w)\n            y1 = random.randint(0, h - occ_h)\n            # Draw black rectangle as obstruction\n            img_pil = img.copy()\n            from PIL import ImageDraw\n            draw = ImageDraw.Draw(img_pil)\n            draw.rectangle([x1, y1, x1+occ_w, y1+occ_h], fill=(0,0,0))\n            img = img_pil\n        return img\nprint(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.374894Z","iopub.execute_input":"2025-09-29T19:17:23.375635Z","iopub.status.idle":"2025-09-29T19:17:23.389201Z","shell.execute_reply.started":"2025-09-29T19:17:23.375611Z","shell.execute_reply":"2025-09-29T19:17:23.388512Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((300, 300)),  # Resize PIL image\n\n    transforms.RandomApply([RandomImageQuality()], p=0.2),\n    transforms.RandomApply([transforms.ColorJitter(\n        brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05\n    )], p=0.3),\n    transforms.RandomApply([transforms.RandomRotation(degrees=30)], p=0.3),\n    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n    RandomOcclusion(p=0.3, size=(20,50)),\n\n    transforms.ToTensor(),  # Convert PIL -> Tensor\n\n   # transforms.RandomErasing(p=0.2, scale=(0.02,0.2), ratio=(0.3,3.3)),  # Tensor only\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((300, 300)),\n    transforms.ToTensor()\n])\n\nprint(\"✅ Transforms ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.390938Z","iopub.execute_input":"2025-09-29T19:17:23.391155Z","iopub.status.idle":"2025-09-29T19:17:23.405308Z","shell.execute_reply.started":"2025-09-29T19:17:23.391137Z","shell.execute_reply":"2025-09-29T19:17:23.404642Z"}},"outputs":[{"name":"stdout","text":"✅ Transforms ready\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ Using device:\", device)\n\n# --- Assign folds for K-Fold cross-validation ---\nK = 5\nskf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\ndf[\"fold\"] = -1\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"image_path\"], df[\"breed\"])):\n    df.loc[val_idx, \"fold\"] = fold\n\n# Save fold info\ndf.to_csv(csv_path, index=False)\nprint(f\"✅ Assigned {K} folds for cross-validation\")\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.405993Z","iopub.execute_input":"2025-09-29T19:17:23.406301Z","iopub.status.idle":"2025-09-29T19:17:23.430681Z","shell.execute_reply.started":"2025-09-29T19:17:23.406275Z","shell.execute_reply":"2025-09-29T19:17:23.430164Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n✅ Assigned 5 folds for cross-validation\nDONE\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"class FoldDataset(Dataset):\n    \"\"\"\n    Custom dataset for K-Fold training.\n    \"\"\"\n    def __init__(self, df, fold, train=True, transform=None):\n        self.transform = transform\n        self.classes = sorted(df[\"breed\"].unique())\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n        self.data = df[df[\"fold\"] != fold] if train else df[df[\"fold\"] == fold]\n        self.data = self.data.reset_index(drop=True)\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        img_path = os.path.join(merged_base, row[\"image_path\"])\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        label = self.class_to_idx[row[\"breed\"]]\n        return img, label\n    print(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.431312Z","iopub.execute_input":"2025-09-29T19:17:23.431569Z","iopub.status.idle":"2025-09-29T19:17:23.437737Z","shell.execute_reply.started":"2025-09-29T19:17:23.431544Z","shell.execute_reply":"2025-09-29T19:17:23.437012Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"val_fold = 0  \n\n# Training set = all folds except 0\ntrain_df = df[df[\"fold\"] != val_fold].reset_index(drop=True)\n\n# Validation set = only fold 0\nval_df   = df[df[\"fold\"] == val_fold].reset_index(drop=True)\n\ntrain_dataset = FoldDataset(train_df, fold=val_fold, train=True, transform=train_transforms)\nval_dataset   = FoldDataset(val_df, fold=val_fold, train=False, transform=val_transforms)\n\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.438476Z","iopub.execute_input":"2025-09-29T19:17:23.438858Z","iopub.status.idle":"2025-09-29T19:17:23.450131Z","shell.execute_reply.started":"2025-09-29T19:17:23.438834Z","shell.execute_reply":"2025-09-29T19:17:23.449501Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"\nbatch_size = 16  # or whatever fits in Kaggle GPU memory\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:17:23.450895Z","iopub.execute_input":"2025-09-29T19:17:23.451158Z","iopub.status.idle":"2025-09-29T19:17:23.460322Z","shell.execute_reply.started":"2025-09-29T19:17:23.451138Z","shell.execute_reply":"2025-09-29T19:17:23.459595Z"}},"outputs":[{"name":"stdout","text":"DONE\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"best_val_acc = 0.0  # track best validation accuracy\n\n# --- Load pretrained EfficientNet-B3 ---\nmodel = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n\n# Freeze all layers initially\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace classifier to match number of cow breeds\nnum_classes = len(df[\"breed\"].unique())\nmodel.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\nmodel = model.to(device)\n\n# --- Criterion ---\ncriterion = nn.CrossEntropyLoss()\n\n# --- Gradual unfreeze schedule ---\n# Each tuple: (phase_name, list of layers to unfreeze, learning_rate, epochs)\nphases = [\n    (\"Phase 1: Train classifier only\", [model.classifier], 1e-3, 20),\n    (\"Phase 2: Unfreeze last block (features[7])\", [model.features[7]], 1e-4, 10),\n    (\"Phase 3: Unfreeze second-to-last block (features[6])\", [model.features[6]], 1e-5, 5),\n    (\"Phase 4: Unfreeze third-to-last block (features[5])\", [model.features[5]], 5e-6, 5)\n]\n\nbest_val_loss = float('inf')\nbest_model_path = \"/kaggle/working/best_model.pth\"\n\n# --- Training loop with gradual unfreezing ---\nfor phase_name, layers_to_unfreeze, lr, epochs in phases:\n    # Unfreeze specified layers\n    for layer in layers_to_unfreeze:\n        for param in layer.parameters():\n            param.requires_grad = True\n\n    # Optimizer for trainable parameters\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Scheduler reduces LR if val_loss plateaus\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\n    print(f\"\\n=== {phase_name} ===\")\n\n    for epoch in range(epochs):\n        # --- Training ---\n        model.train()\n        train_loss = 0\n        all_train_labels = []\n        all_train_preds = []\n\n        for images, labels in train_loader:\n            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * images.size(0)\n            preds = outputs.argmax(dim=1)\n            all_train_labels.extend(labels.cpu().numpy())\n            all_train_preds.extend(preds.cpu().numpy())\n\n        train_loss /= len(train_loader.dataset)\n        train_acc = accuracy_score(all_train_labels, all_train_preds)\n        train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n\n        # --- Validation ---\n        model.eval()\n        val_loss = 0\n        all_val_labels = []\n        all_val_preds = []\n\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n\n                preds = outputs.argmax(dim=1)\n                all_val_labels.extend(labels.cpu().numpy())\n                all_val_preds.extend(preds.cpu().numpy())\n\n        val_loss /= len(val_loader.dataset)\n        val_acc = accuracy_score(all_val_labels, all_val_preds)\n        val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n\n        # Step scheduler\n        scheduler.step(val_loss)\n\n        print(f\"Epoch {epoch+1}/{epochs} | \"\n              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"✅ Best model updated (Val Acc: {best_val_acc:.4f})\")\n\nprint(f\"\\n🏆 Training complete. Best model saved at {best_model_path}\")\n\nprint(\"DONE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:34:58.112148Z","iopub.execute_input":"2025-09-29T19:34:58.112480Z","iopub.status.idle":"2025-09-29T19:46:41.404645Z","shell.execute_reply.started":"2025-09-29T19:34:58.112455Z","shell.execute_reply":"2025-09-29T19:46:41.403826Z"}},"outputs":[{"name":"stdout","text":"\n=== Phase 1: Train classifier only ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 | Train Loss: 1.8275, Acc: 0.4355, F1: 0.4100 | Val Loss: 8.9322, Acc: 0.6465, F1: 0.6340\n✅ Best model updated (Val Acc: 0.6465)\nEpoch 2/20 | Train Loss: 1.3278, Acc: 0.6023, F1: 0.5919 | Val Loss: 2.2122, Acc: 0.6590, F1: 0.6414\n✅ Best model updated (Val Acc: 0.6590)\nEpoch 3/20 | Train Loss: 1.1646, Acc: 0.6303, F1: 0.6226 | Val Loss: 3.2157, Acc: 0.6821, F1: 0.6758\n✅ Best model updated (Val Acc: 0.6821)\nEpoch 4/20 | Train Loss: 1.0641, Acc: 0.6677, F1: 0.6635 | Val Loss: 2.9779, Acc: 0.6856, F1: 0.6795\n✅ Best model updated (Val Acc: 0.6856)\nEpoch 5/20 | Train Loss: 1.0029, Acc: 0.6779, F1: 0.6740 | Val Loss: 0.8917, Acc: 0.7052, F1: 0.7015\n✅ Best model updated (Val Acc: 0.7052)\nEpoch 6/20 | Train Loss: 0.9825, Acc: 0.6882, F1: 0.6856 | Val Loss: 3.6441, Acc: 0.7211, F1: 0.7172\n✅ Best model updated (Val Acc: 0.7211)\nEpoch 7/20 | Train Loss: 0.9353, Acc: 0.7033, F1: 0.6996 | Val Loss: 6.0147, Acc: 0.7229, F1: 0.7217\n✅ Best model updated (Val Acc: 0.7229)\nEpoch 8/20 | Train Loss: 0.9343, Acc: 0.6917, F1: 0.6899 | Val Loss: 3.4500, Acc: 0.7211, F1: 0.7122\nEpoch 9/20 | Train Loss: 0.8778, Acc: 0.7122, F1: 0.7090 | Val Loss: 12.2850, Acc: 0.7442, F1: 0.7408\n✅ Best model updated (Val Acc: 0.7442)\nEpoch 10/20 | Train Loss: 0.8692, Acc: 0.7171, F1: 0.7142 | Val Loss: 6.3241, Acc: 0.7211, F1: 0.7137\nEpoch 11/20 | Train Loss: 0.8585, Acc: 0.7091, F1: 0.7058 | Val Loss: 6.5839, Acc: 0.7265, F1: 0.7202\nEpoch 12/20 | Train Loss: 0.8556, Acc: 0.7282, F1: 0.7252 | Val Loss: 3.2739, Acc: 0.7265, F1: 0.7223\nEpoch 13/20 | Train Loss: 0.8485, Acc: 0.7175, F1: 0.7141 | Val Loss: 0.8311, Acc: 0.7407, F1: 0.7366\nEpoch 14/20 | Train Loss: 0.8712, Acc: 0.7171, F1: 0.7143 | Val Loss: 5.5114, Acc: 0.7442, F1: 0.7418\nEpoch 15/20 | Train Loss: 0.8465, Acc: 0.7278, F1: 0.7252 | Val Loss: 10.8010, Acc: 0.7460, F1: 0.7429\n✅ Best model updated (Val Acc: 0.7460)\nEpoch 16/20 | Train Loss: 0.8103, Acc: 0.7349, F1: 0.7328 | Val Loss: 3.6059, Acc: 0.7282, F1: 0.7224\nEpoch 17/20 | Train Loss: 0.8317, Acc: 0.7282, F1: 0.7256 | Val Loss: 9.7112, Acc: 0.7371, F1: 0.7331\nEpoch 18/20 | Train Loss: 0.8625, Acc: 0.7091, F1: 0.7072 | Val Loss: 3.8281, Acc: 0.7336, F1: 0.7281\nEpoch 19/20 | Train Loss: 0.8486, Acc: 0.7189, F1: 0.7165 | Val Loss: 11.7917, Acc: 0.7300, F1: 0.7258\nEpoch 20/20 | Train Loss: 0.8391, Acc: 0.7269, F1: 0.7244 | Val Loss: 1.2497, Acc: 0.7496, F1: 0.7463\n✅ Best model updated (Val Acc: 0.7496)\n\n=== Phase 2: Unfreeze last block (features[7]) ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 0.7964, Acc: 0.7500, F1: 0.7478 | Val Loss: 6.1044, Acc: 0.7460, F1: 0.7438\nEpoch 2/10 | Train Loss: 0.7239, Acc: 0.7633, F1: 0.7615 | Val Loss: 23.6262, Acc: 0.7798, F1: 0.7779\n✅ Best model updated (Val Acc: 0.7798)\nEpoch 3/10 | Train Loss: 0.6714, Acc: 0.7754, F1: 0.7742 | Val Loss: 0.6988, Acc: 0.7869, F1: 0.7845\n✅ Best model updated (Val Acc: 0.7869)\nEpoch 4/10 | Train Loss: 0.6330, Acc: 0.7967, F1: 0.7958 | Val Loss: 5.0366, Acc: 0.7780, F1: 0.7741\nEpoch 5/10 | Train Loss: 0.5976, Acc: 0.7998, F1: 0.7990 | Val Loss: 3.2269, Acc: 0.7869, F1: 0.7835\nEpoch 6/10 | Train Loss: 0.5289, Acc: 0.8283, F1: 0.8272 | Val Loss: 0.6671, Acc: 0.7851, F1: 0.7790\nEpoch 7/10 | Train Loss: 0.4913, Acc: 0.8354, F1: 0.8344 | Val Loss: 2.4762, Acc: 0.7780, F1: 0.7752\nEpoch 8/10 | Train Loss: 0.4622, Acc: 0.8474, F1: 0.8468 | Val Loss: 5.3892, Acc: 0.7869, F1: 0.7841\nEpoch 9/10 | Train Loss: 0.4667, Acc: 0.8407, F1: 0.8397 | Val Loss: 19.4889, Acc: 0.7691, F1: 0.7676\nEpoch 10/10 | Train Loss: 0.4000, Acc: 0.8763, F1: 0.8761 | Val Loss: 15.3972, Acc: 0.7833, F1: 0.7816\n\n=== Phase 3: Unfreeze second-to-last block (features[6]) ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | Train Loss: 0.3875, Acc: 0.8763, F1: 0.8756 | Val Loss: 3.0704, Acc: 0.7922, F1: 0.7905\n✅ Best model updated (Val Acc: 0.7922)\nEpoch 2/5 | Train Loss: 0.3726, Acc: 0.8835, F1: 0.8827 | Val Loss: 2.7424, Acc: 0.7869, F1: 0.7854\nEpoch 3/5 | Train Loss: 0.3664, Acc: 0.8821, F1: 0.8817 | Val Loss: 15.3867, Acc: 0.7851, F1: 0.7830\nEpoch 4/5 | Train Loss: 0.3495, Acc: 0.8923, F1: 0.8921 | Val Loss: 5.8901, Acc: 0.7957, F1: 0.7943\n✅ Best model updated (Val Acc: 0.7957)\nEpoch 5/5 | Train Loss: 0.3792, Acc: 0.8835, F1: 0.8827 | Val Loss: 2.0935, Acc: 0.7886, F1: 0.7861\n\n=== Phase 4: Unfreeze third-to-last block (features[5]) ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | Train Loss: 0.3487, Acc: 0.8928, F1: 0.8923 | Val Loss: 15.7164, Acc: 0.7833, F1: 0.7823\nEpoch 2/5 | Train Loss: 0.3623, Acc: 0.8888, F1: 0.8881 | Val Loss: 8.5515, Acc: 0.7869, F1: 0.7854\nEpoch 3/5 | Train Loss: 0.3464, Acc: 0.8888, F1: 0.8882 | Val Loss: 4.0841, Acc: 0.7798, F1: 0.7772\nEpoch 4/5 | Train Loss: 0.3477, Acc: 0.8915, F1: 0.8909 | Val Loss: 1.1316, Acc: 0.7904, F1: 0.7880\nEpoch 5/5 | Train Loss: 0.3294, Acc: 0.8968, F1: 0.8962 | Val Loss: 1.4125, Acc: 0.7833, F1: 0.7821\n\n🏆 Training complete. Best model saved at /kaggle/working/best_model.pth\nDONE\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import torch\n\n# Map to available device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load model weights safely\nmodel.load_state_dict(torch.load(best_model_path, map_location=device))\nmodel.to(device)  # move model to the device\nmodel.eval()\n\nprint(\"✅ Model loaded and ready for inference on\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:46:41.406118Z","iopub.execute_input":"2025-09-29T19:46:41.406434Z","iopub.status.idle":"2025-09-29T19:46:41.637780Z","shell.execute_reply.started":"2025-09-29T19:46:41.406411Z","shell.execute_reply":"2025-09-29T19:46:41.636991Z"}},"outputs":[{"name":"stdout","text":"✅ Model loaded and ready for inference on cuda:0\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# ---------------- Prepare export folder ----------------\nimport os, shutil, json, subprocess\n\nmerged_base = '/kaggle/working/merged_dataset'\nbest_model_path = '/kaggle/working/best_model.pth'\nexport_dir = '/kaggle/working/export_dataset'\nos.makedirs(export_dir, exist_ok=True)\n\n# Copy preprocessed dataset\nif not os.path.exists(merged_base):\n    raise FileNotFoundError(f\"Preprocessed dataset not found at {merged_base} - run preprocessing first.\")\nshutil.copytree(merged_base, os.path.join(export_dir, 'merged_dataset'), dirs_exist_ok=True)\n\n# Copy best model if exists\nif os.path.exists(best_model_path):\n    shutil.copy2(best_model_path, os.path.join(export_dir, os.path.basename(best_model_path)))\nelse:\n    print(\"⚠ best_model.pth not found — continuing without model.\")\n\n# Path to metadata\nmetadata_path = os.path.join(export_dir, 'dataset-metadata.json')\n\n# Create minimal metadata if missing\nif not os.path.exists(metadata_path):\n    meta = {\n        \"title\": \"bovine dataset and model using yolo 12breeds arpit\",\n        \"id\": \"<username>/bovine_dataset_and_model\",  # placeholder\n        \"licenses\": [{\"name\": \"CC0-1.0\"}],\n        \"isPrivate\": True,\n        \"subtitle\": \"SAM-cropped bovine + calf images (optional model)\",\n        \"description\": \"Preprocessed dataset (Segment-Anything crops) plus optional trained model.\"\n    }\n    with open(metadata_path, 'w') as f:\n        json.dump(meta, f, indent=2)\n    print(\"✅ Created minimal dataset-metadata.json\")\n\nprint(\"✅ Export folder prepared at:\", export_dir)\nprint(\"Contents preview:\", os.listdir(export_dir)[:20])\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:46:41.638726Z","iopub.execute_input":"2025-09-29T19:46:41.638986Z","iopub.status.idle":"2025-09-29T19:46:42.017234Z","shell.execute_reply.started":"2025-09-29T19:46:41.638968Z","shell.execute_reply":"2025-09-29T19:46:42.016574Z"}},"outputs":[{"name":"stdout","text":"✅ Created minimal dataset-metadata.json\n✅ Export folder prepared at: /kaggle/working/export_dataset\nContents preview: ['dataset-metadata.json', 'best_model.pth', 'merged_dataset']\nDone\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# ---------------- Kaggle CLI setup ----------------\nkaggle_json_src = '/kaggle/input/kaggle-api/kaggle.json'\nkaggle_local = os.path.expanduser('~/.kaggle/kaggle.json')\n\n# Install kaggle CLI\n!pip install --quiet kaggle\n\n# Copy kaggle.json\nif os.path.exists(kaggle_local):\n    print(\"Found existing ~/.kaggle/kaggle.json — using it.\")\nelif os.path.exists(kaggle_json_src):\n    os.makedirs(os.path.dirname(kaggle_local), exist_ok=True)\n    shutil.copy2(kaggle_json_src, kaggle_local)\n    os.chmod(kaggle_local, 0o600)\n    print(f\"Copied kaggle.json from {kaggle_json_src} -> {kaggle_local}\")\nelse:\n    raise FileNotFoundError(\"kaggle.json not found. Upload it via notebook UI and re-run this cell.\")\n\n# Read username\nwith open(kaggle_local, 'r') as f:\n    kg = json.load(f)\nusername = kg.get('username') or kg.get('user') or kg.get('email') or None\n\n# Update metadata id if possible\nslug = 'bovine-dataset-and-model-using-yolo-12breeds-arpit'\nif username:\n    with open(metadata_path, 'r') as f:\n        meta = json.load(f)\n    meta['id'] = f\"{username}/{slug}\"\n    with open(metadata_path, 'w') as f:\n        json.dump(meta, f, indent=2)\n    print(f\"Dataset id set to: {meta['id']}\")\nelse:\n    print(\"⚠ Could not determine username — dataset id in metadata will remain placeholder.\")\n\n# ---------------- Create or version Kaggle dataset ----------------\ncreate_cmd = f\"kaggle datasets create -p {export_dir} --dir-mode zip\"\nprint(\"Running:\", create_cmd)\nres = subprocess.run(create_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n\nif res.returncode == 0:\n    print(\"✅ Dataset created successfully.\")\n    print(res.stdout)\nelse:\n    print(\"Create failed; attempting to create a new version...\")\n    version_cmd = f\"kaggle datasets version -p {export_dir} -m \\\"Update: SAM preproc + optional model\\\" --dir-mode zip --force\"\n    print(\"Running:\", version_cmd)\n    res2 = subprocess.run(version_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    if res2.returncode == 0:\n        print(\"✅ Dataset version created successfully.\")\n        print(res2.stdout)\n    else:\n        print(\"❌ Both create and version failed.\")\n        print(\"CREATE stderr:\\n\", res.stderr)\n        print(\"VERSION stderr:\\n\", res2.stderr)\n        raise RuntimeError(\"Failed to create/version Kaggle dataset. Check kaggle.json and metadata id.\")\n\n# ---------------- Dataset URL ----------------\nif username:\n    print(\"\\nDataset should be available at:\")\n    print(f\"https://www.kaggle.com/{username}/{slug}\")\nelse:\n    print(\"\\nDataset created/updated but username unknown. Check Kaggle web UI.\")\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T19:46:42.018471Z","iopub.execute_input":"2025-09-29T19:46:42.018683Z","iopub.status.idle":"2025-09-29T19:46:52.013980Z","shell.execute_reply.started":"2025-09-29T19:46:42.018668Z","shell.execute_reply":"2025-09-29T19:46:52.013073Z"}},"outputs":[{"name":"stdout","text":"Copied kaggle.json from /kaggle/input/kaggle-api/kaggle.json -> /root/.kaggle/kaggle.json\nDataset id set to: hacker108/bovine-dataset-and-model-using-yolo-12breeds-arpit\nRunning: kaggle datasets create -p /kaggle/working/export_dataset --dir-mode zip\n✅ Dataset created successfully.\nStarting upload for file best_model.pth\nUpload successful: best_model.pth (41MB)\nStarting upload for file merged_dataset.zip\nUpload successful: merged_dataset.zip (90MB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/hacker108/bovine-dataset-and-model-using-yolo-12breeds-arpit\n\n\nDataset should be available at:\nhttps://www.kaggle.com/hacker108/bovine-dataset-and-model-using-yolo-12breeds-arpit\nDone\n","output_type":"stream"}],"execution_count":36}]}